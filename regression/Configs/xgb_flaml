class CFG:
    #Random seed
    SEED = 42
    
    BASE = '/home/ml/Jupyter_root/123/CRAB_AGE/'
    
    n_splits = 5 
    
    max_iter = None
    pca_n=2
    log_file_name_xgb = 'settings_xgb.log'  # flaml log file

    p_train_time_xgb = 3600
    
    metric = "mae"
    eval_method = "cv"

X_train_shape = X_train.shape[0]
FL_automl_settings_xgb = {      
        "time_budget": CFG.p_train_time_xgb,  # in seconds
        'max_iter': CFG.max_iter,
        "metric": CFG.metric,
        "task": 'regression',
        "verbose": 3,
        "n_jobs": -1,
        "log_file_name": CFG.log_file_name_xgb,  # flaml log file
        "eval_method": CFG.eval_method,
        "estimator_list": [
            "xgboost","lgbm"
        ],        
        
        "auto_augment":True,
        "n_splits": CFG.n_splits,
        "ensemble" : True,
        "ensemble": {
        "final_estimator": LADRegression(),
        "passthrough": True,
    },
        "sample_weight": np.ones(X_train_shape),
        "seed":1,
        "n_concurrent_trials": 16,
        "keep_search_state": True, # needed if you want to keep the cross validation information
    
        "custom_hp": {
            "xgboost":{
                    "colsample_bytree":{"domain":tune.choice(np.arange(0.000001, 1, 0.1))},
                    "colsample_bylevel":{"domain":tune.choice(np.arange(0.000001, 1, 0.1))},
                    "colsample_bynode":{"domain":tune.choice(np.arange(0.000001, 1, 0.01))},
                    "objective":{"domain":'reg:pseudohubererror'},
                    "tree_method":{"domain":'gpu_hist'},
                    "class_weight":{"domain":'balanced'},
                    "random_state":{"domain":CFG.SEED},
                    
            },
            "lgbm":{
                "boosting_type":{"domain":'dart'},
                "seed": {"domain": CFG.SEED},
            }        
        }}
